{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T07:37:15.239784Z",
     "start_time": "2024-02-15T07:37:14.888813Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb9c70bca047c87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T07:37:15.319831Z",
     "start_time": "2024-02-15T07:37:15.241017Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac5d8cb9d03d16b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T07:37:15.335418Z",
     "start_time": "2024-02-15T07:37:15.323288Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessing(df,\n",
    "                  columns_to_remove = None,\n",
    "                  ratio = False,\n",
    "                  smoothing = False,\n",
    "                  collinearity = False, \n",
    "                  log = False,\n",
    "                  aggregation = 0,\n",
    "                  statistics = False,\n",
    "                  differencing = 0, \n",
    "                  seasonality = False, \n",
    "                  normalize = True,\n",
    "                  load_scaler = False,\n",
    "                  save = True,\n",
    "                  continual = False):\n",
    "    \n",
    "    if columns_to_remove is None:\n",
    "        columns_to_remove = []\n",
    "    LENGTH = len(df)\n",
    "\n",
    "    # Processed dataset path\n",
    "    df_path = ('df_' + str(len(columns_to_remove)) + '_' + str(ratio) + '_' + str(smoothing) + '_' + str(collinearity) + '_' + str(log) + '_' + str(aggregation) + '_' + str(statistics) + '_' + str(differencing) + '_' + str(seasonality) + '_' + str(normalize))  \n",
    "\n",
    "    # Remove columns not used in training\n",
    "    if columns_to_remove:\n",
    "        for column in columns_to_remove:\n",
    "            if column in df.columns:\n",
    "                df = df.drop(columns = column)\n",
    "\n",
    "    # Analyze collinearity to remove useless features\n",
    "    if collinearity:\n",
    "        correlation_matrix = df.corr()\n",
    "        plt.figure(figsize = (12, 10))\n",
    "        sns.heatmap(correlation_matrix, annot = True, fmt = \".2f\", cmap = 'coolwarm', square = True, linewidths = .5, cbar_kws = {\"shrink\": .5})\n",
    "        plt.savefig('features_corr_matrix.png', dpi = 300)\n",
    "        plt.close()\n",
    "        features = df.columns\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data[\"Feature\"] = df.columns\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(len(features))]\n",
    "        # Remove features with VIF > 10 from df\n",
    "        features_to_remove = vif_data[vif_data['VIF'] > 10]['Feature']\n",
    "        df = df.drop(columns = features_to_remove)\n",
    "        \n",
    "    # Compute ratio between rides and requested\n",
    "    if ratio:\n",
    "        subset = ['rides_canceled', 'rides_accepted', 'rides_rejections', 'rides_completed']\n",
    "        for column in subset:\n",
    "            df[column] = df[column].astype(float)\n",
    "            for i in range(0, len(df[column])):\n",
    "                if df.loc[i, 'rides_requested'] != 0:\n",
    "                    df.loc[i, column] = df.loc[i, column]/df.loc[i, 'rides_requested']\n",
    "                else:\n",
    "                    df.loc[i, column] = 0\n",
    "                     \n",
    "    # Compute log transformation\n",
    "    if log:\n",
    "        subset = ['rides_canceled', 'rides_accepted', 'rides_rejections', 'rides_requested', 'rides_completed']\n",
    "        for column in subset:\n",
    "            if column in df.columns:\n",
    "                df[column] = df[column].astype(float)\n",
    "                for i in range(0, len(df)):\n",
    "                    if df.loc[i, column] != 0:\n",
    "                        df.loc[i, column] = np.log(df.loc[i, column])\n",
    "            \n",
    "    df = df.dropna().reset_index(drop = True)\n",
    "                    \n",
    "    # Pay attention to seasonalities\n",
    "    if seasonality:\n",
    "        for column in df.columns:\n",
    "            if column == 'avg_current_error_ride_distance':\n",
    "                df[column] = df[column].diff(periods = 90)\n",
    "            elif column == 'avg_speed_max_speed':\n",
    "                df[column] = df[column].diff(periods = 90)\n",
    "            elif column == 'avg_speed_kmh':\n",
    "                df[column] = df[column].diff(periods = 90)\n",
    "            elif column == 'avg_remaining_distance_covered':\n",
    "                df[column] = df[column].diff(periods = 90)\n",
    "            elif column == 'avg_surge_multiplier':\n",
    "                df[column] = df[column].diff(periods = 300)\n",
    "            elif column == 'moving_drivers':\n",
    "                df[column] = df[column].diff(periods = 900)\n",
    "            \n",
    "    df = df.dropna().reset_index(drop = True)\n",
    "\n",
    "    # Aggregate consecutive observations by mean (useful for smoothing, reducing outliers and noise)\n",
    "    if aggregation != 0 and statistics == False:\n",
    "        for column in df.columns:\n",
    "            if column not in ['avg_ratio_cust_driv', 'avg_dynamic_greediness']:\n",
    "                df[column] = np.around(df[column].astype(float), 5)\n",
    "                df[column] = df[column].rolling(window = aggregation).mean()\n",
    "        df = df[aggregation:].reset_index(drop = True)\n",
    "    \n",
    "    # Compute additional statistics for each feature\n",
    "    if aggregation != 0 and statistics == True:\n",
    "        for column in df.columns:\n",
    "            df[column] = np.around(df[column].astype(float), 5)\n",
    "        df_ = copy.deepcopy(df)\n",
    "        for column in df_.columns:\n",
    "            df[column] = df[column].rolling(window = aggregation).mean()\n",
    "            new_cols = []\n",
    "            new_cols.append(pd.Series(df_[column].rolling(window = aggregation).median(), name = column + '_median'))\n",
    "            new_cols.append(pd.Series(df_[column].rolling(window = aggregation).std(), name = column + '_std'))\n",
    "            new_cols.append(pd.Series(df_[column].rolling(window = aggregation).quantile(0.25), name = column + '_q25'))\n",
    "            new_cols.append(pd.Series(df_[column].rolling(window = aggregation).quantile(0.75), name = column + '_q75'))\n",
    "            new_cols.append(pd.Series(np.around(df_[column], 4).rolling(window = aggregation).apply(calc_kurtosis, raw = True), name = column + '_kurtosis'))\n",
    "            new_cols.append(pd.Series(np.around(df_[column], 4).rolling(window = aggregation).apply(calc_skewness, raw = True), name = column + '_skewness'))\n",
    "            df = pd.concat([df] + new_cols, axis = 1)\n",
    "        df = df[aggregation:].reset_index(drop = True)\n",
    "            \n",
    "    df = df.dropna().reset_index(drop = True)\n",
    "        \n",
    "    # Differentiate the time series\n",
    "    if differencing != 0:\n",
    "        if not seasonality:\n",
    "            for column in df.columns:\n",
    "                df[column] = df[column].diff(periods = differencing)\n",
    "        else:\n",
    "            for column in df.columns:\n",
    "                if column not in ['avg_surge_multiplier', 'moving_drivers', 'avg_current_error_ride_distance', 'avg_speed_max_speed', 'avg_speed_kmh', 'avg_remaining_distance_covered']:\n",
    "                    df[column] = df[column].diff(periods = differencing)\n",
    "    \n",
    "    df = df.dropna().reset_index(drop = True)\n",
    "         \n",
    "    # Smooth noisy columns           \n",
    "    if smoothing:\n",
    "        subset = ['rides_canceled', 'rides_accepted', 'rides_rejections', 'rides_requested', 'rides_completed']\n",
    "        for column in subset:\n",
    "            df[column] = gaussian_filter1d(df[column], sigma = 2)\n",
    "\n",
    "    df = df.dropna().reset_index(drop = True)\n",
    "    \n",
    "    # Removing stabilization time and final time\n",
    "    removed = LENGTH - len(df)\n",
    "    df = df[5400-removed:-1800]\n",
    "    \n",
    "    # Normalize the values of each time series between 0 and 1\n",
    "    if normalize:\n",
    "        if load_scaler:\n",
    "            if continual:\n",
    "                with open('scalers/continual_' + df_path + '.pkl', 'rb') as file:\n",
    "                    scaler = pickle.load(file)\n",
    "            else:\n",
    "                with open('scalers/' + df_path + '.pkl', 'rb') as file:\n",
    "                    scaler = pickle.load(file)\n",
    "            df = normalization(scaler, df) \n",
    "        else:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(df.values)\n",
    "            df = normalization(scaler, df)\n",
    "            if continual:\n",
    "                with open('scalers/continual_' + df_path + '.pkl', 'wb') as file:\n",
    "                    pickle.dump(scaler, file)\n",
    "            else:\n",
    "                with open('scalers/' + df_path + '.pkl', 'wb') as file:\n",
    "                    pickle.dump(scaler, file)\n",
    "        \n",
    "    # Save specific dataset and return\n",
    "    if save:\n",
    "        if continual:\n",
    "            df.to_csv('datasets/proc/continual/' + df_path + '.csv', index = False, header = True)\n",
    "        else:\n",
    "            df.to_csv('datasets/proc/' + df_path + '.csv', index = False, header = True)\n",
    "    if normalize:\n",
    "        return df, scaler\n",
    "    else:\n",
    "        return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
