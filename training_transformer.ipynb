{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-10T15:58:25.320047Z",
     "start_time": "2025-02-10T15:58:22.114546Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, MultiHeadAttention, Add, LayerNormalization\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "seed = 123\n",
    "tf.keras.utils.set_random_seed(seed)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "%run utils.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-10T15:58:25.488729Z",
     "start_time": "2025-02-10T15:58:25.324299Z"
    }
   },
   "id": "601d6e1fae9ad1ab",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def training_transformer_autoencoder(\n",
    "    train_data,\n",
    "    val_data,\n",
    "    n_heads = 8,\n",
    "    d_model = 128,\n",
    "    num_encoder_layers = 1,\n",
    "    num_decoder_layers = 1,\n",
    "    feed_forward_dim = 256,\n",
    "    dropout_rate = 0.2,\n",
    "    learning_rate = 0.0001,\n",
    "    n_epochs = 500,\n",
    "    batch_size = 32,\n",
    "    window_size = 20,\n",
    "    metric = 'mse',\n",
    "    plot = True,\n",
    "    save = True\n",
    "):\n",
    "\n",
    "    # Get the number of features from the training data\n",
    "    number_of_features = train_data.shape[2]\n",
    "    # Optimizer and loss\n",
    "    opt = Adam(learning_rate = learning_rate)\n",
    "    loss_metric = MeanSquaredError() if metric == 'mse' else MeanAbsoluteError()\n",
    "\n",
    "    # Positional Encoding\n",
    "    def positional_encoding(position, d_mod):\n",
    "        \"\"\"\n",
    "        Generate positional encoding for the input sequences\n",
    "        This helps the model encode the order of the sequence elements\n",
    "        \"\"\"\n",
    "        angle_rads = np.arange(position)[:, np.newaxis] / np.power(\n",
    "            10000, (2 * (np.arange(d_mod)[np.newaxis, :] // 2)) / np.float32(d_mod)\n",
    "        )\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2]) # Apply sine to even indices\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2]) # Apply cosine to odd indices\n",
    "        pos_enc = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_enc, dtype = tf.float32)\n",
    "\n",
    "    # Transformer Block\n",
    "    def transformer_block(x_input, num_heads, feed_forward_d, d_mod, dropout):\n",
    "        \"\"\"\n",
    "        Transformer block consisting of multi-head self-attention,\n",
    "        feed-forward layers, layer normalization, and dropout\n",
    "        \"\"\"\n",
    "        # Multi-Head Attention\n",
    "        attention_output = MultiHeadAttention(num_heads = num_heads, key_dim = d_mod)(x_input, x_input)\n",
    "        attention_output = Dropout(dropout)(attention_output)\n",
    "        out1 = Add()([x_input, attention_output]) # Add residual connection\n",
    "        out1 = LayerNormalization(epsilon = 1e-6)(out1) # Normalize the output\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        ffn_output = Dense(feed_forward_d, activation = 'relu')(out1)\n",
    "        ffn_output = Dense(d_mod)(ffn_output)\n",
    "        ffn_output = Dropout(dropout)(ffn_output)\n",
    "        out2 = Add()([out1, ffn_output]) # Add residual connection\n",
    "        out2 = LayerNormalization(epsilon = 1e-6)(out2) # Normalize the output\n",
    "\n",
    "        return out2\n",
    "\n",
    "    # Add Noise\n",
    "    def add_noise(data, noise_level):\n",
    "        \"\"\"\n",
    "        Add Gaussian noise to the data for denoising autoencoder training\n",
    "        \"\"\"\n",
    "        noisy_data = data + np.random.normal(scale = noise_level, size = data.shape)\n",
    "        return noisy_data\n",
    "\n",
    "    # Build the Autoencoder Model\n",
    "    inputs = Input(shape = (window_size, number_of_features))\n",
    "\n",
    "    # Apply Positional Encoding\n",
    "    pos_encoding = positional_encoding(window_size, d_model)\n",
    "    x = Dense(d_model)(inputs) # Map inputs to d_model dimensions\n",
    "    x += pos_encoding # Add positional encoding\n",
    "\n",
    "    # Encoder\n",
    "    for _ in range(num_encoder_layers):\n",
    "        x = transformer_block(x, n_heads, feed_forward_dim, d_model, dropout_rate)\n",
    "    encoder_output = x # Final encoder output\n",
    "\n",
    "    # Decoder\n",
    "    x = encoder_output\n",
    "    for _ in range(num_decoder_layers):\n",
    "        x = transformer_block(x, n_heads, feed_forward_dim, d_model, dropout_rate)\n",
    "\n",
    "    # Final Dense Layer\n",
    "    outputs = Dense(number_of_features, activation = 'sigmoid')(x) # Map back to original feature space\n",
    "\n",
    "    # Compile the Model\n",
    "    autoencoder = Model(inputs, outputs)\n",
    "    autoencoder.compile(optimizer = opt, loss = loss_metric)\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 50, verbose = 1, mode = 'min', restore_best_weights = True)\n",
    "\n",
    "    # Add noise to training data\n",
    "    noisy_train_data = add_noise(train_data, noise_level = 0.005)\n",
    "\n",
    "    # Train Model\n",
    "    if val_data is not None:\n",
    "        history = autoencoder.fit(\n",
    "            noisy_train_data, val_data,\n",
    "            epochs = n_epochs,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = False,\n",
    "            validation_data = (val_data, val_data),\n",
    "            callbacks = [early_stopping]\n",
    "        )\n",
    "    else:\n",
    "        history = autoencoder.fit(\n",
    "            noisy_train_data, train_data,\n",
    "            epochs = n_epochs,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = False\n",
    "        )\n",
    "\n",
    "    # Generate model path using hyperparameters for easy identification\n",
    "    lr = str(learning_rate).replace('.', '')\n",
    "    dr = str(dropout_rate).replace('.', '')\n",
    "    model_path = f'transformer_autoencoder_{num_encoder_layers}_{n_heads}_{d_model}_{feed_forward_dim}_{dr}_{lr}_{n_epochs}_{metric}_{batch_size}'\n",
    "\n",
    "    # Plot training and validation loss if specified\n",
    "    if plot:\n",
    "        plt.figure(figsize = (10, 5))\n",
    "        plt.plot(history.history['loss'], label = 'Training Loss')\n",
    "        if 'val_loss' in history.history:\n",
    "            plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        loss_save_path = 'losses/' + model_path\n",
    "        plt.savefig(loss_save_path + '.png')\n",
    "        plt.close()\n",
    "\n",
    "    # Save model if specified\n",
    "    if save:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model_save_path = 'models/' + model_path + '.pkl'\n",
    "        with open(model_save_path, 'wb') as file:\n",
    "            pickle.dump(autoencoder, file)\n",
    "\n",
    "    return history, autoencoder"
   ],
   "id": "8ea6d7169b7dcad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_mas_importance(model, dataset):\n",
    "    \"\"\"\n",
    "    Compute MAS importance for each trainable parameter\n",
    "    We pass data, compute the squared L2 norm of the outputs,\n",
    "    and accumulate the absolute gradient for each parameter\n",
    "    \"\"\"\n",
    "    importance = [tf.zeros_like(var, dtype = tf.float32) for var in model.trainable_variables]\n",
    "    n_batches = 0\n",
    "\n",
    "    for x_batch in dataset:\n",
    "        n_batches += 1\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(x_batch, training = False)\n",
    "            loss = tf.reduce_sum(tf.square(outputs))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        for i, g in enumerate(grads):\n",
    "            if g is not None:\n",
    "                importance[i] += tf.abs(g)\n",
    "\n",
    "    # Average over all batches\n",
    "    for i in range(len(importance)):\n",
    "        importance[i] /= float(n_batches)\n",
    "\n",
    "    return importance"
   ],
   "id": "51ec66542a341849"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_current_weights(model):\n",
    "    # Return a list of tf.Tensors that copy the model's current trainable variables\n",
    "    return [tf.identity(v) for v in model.trainable_variables]"
   ],
   "id": "f36ca6d5ffa90d43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def mas_finetune_on_new_task(\n",
    "    model_path,\n",
    "    old_data,         # Data from old training to preserve performance\n",
    "    new_data,         # Data for continual learning\n",
    "    lambda_ = 1.0,    # MAS regularization weight\n",
    "    n_epochs = 50,\n",
    "    batch_size = 32,\n",
    "    learning_rate = 1e-4\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a previously trained Transformer autoencoder and\n",
    "    applies MAS-based fine-tuning on a new task\n",
    "    \"\"\"\n",
    "    save_path = \"models/transformer_autoencoder_MAS.pkl\"\n",
    "\n",
    "    # Optimizer and loss\n",
    "    opt = Adam(learning_rate = learning_rate)\n",
    "    loss_metric = MeanSquaredError()\n",
    "\n",
    "    # Load trained model\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    # Compute old importance using old data and store params\n",
    "    old_dataset = tf.data.Dataset.from_tensor_slices(old_data).batch(batch_size)\n",
    "    old_importance = compute_mas_importance(model, old_dataset)\n",
    "    old_params = get_current_weights(model)\n",
    "\n",
    "    # Create dataset for the new task\n",
    "    new_dataset = tf.data.Dataset.from_tensor_slices((new_data, new_data)).batch(batch_size)\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    "    loss_metric = MeanSquaredError()\n",
    "    loss_history = []\n",
    "\n",
    "    # Train with MAS penalty\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for x_batch, y_batch in new_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(x_batch, training = True)\n",
    "                main_loss = loss_metric(y_batch, predictions)\n",
    "                # MAS regularization\n",
    "                mas_reg = 0.0\n",
    "                for param, old_param, imp in zip(model.trainable_variables, old_params, old_importance):\n",
    "                    mas_reg += tf.reduce_sum(imp * tf.square(param - old_param))\n",
    "                total_loss = main_loss + lambda_ * mas_reg\n",
    "            # Backprop\n",
    "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            epoch_loss += total_loss.numpy()\n",
    "            num_batches += 1\n",
    "        epoch_loss /= num_batches\n",
    "        loss_history.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - loss: {epoch_loss:.6f}\")\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    plt.plot(range(1, n_epochs + 1), loss_history,  label = 'Continual Learning - Training Loss')\n",
    "    plt.title('Updated model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid()\n",
    "    plt.savefig(\"losses/transformer_autoencoder_MAS.png\")\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Save updated model\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    return model"
   ],
   "id": "30fb2dfcd5d96534"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
